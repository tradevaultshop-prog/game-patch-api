import os
import json
import time
import random
import logging
import boto3
import requests
import yaml 
import scrapers 
import concurrent.futures
import hashlib # <-- YENÄ° EKLENDÄ° (Hash kontrolÃ¼ iÃ§in)
import sys # <-- YENÄ° EKLENDÄ° (Mod seÃ§imi iÃ§in)
from datetime import datetime
from dotenv import load_dotenv
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from utils import analyze_with_gemini 

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

s3_client = boto3.client(
    "s3",
    endpoint_url=os.getenv("S3_ENDPOINT_URL"),
    aws_access_key_id=os.getenv("S3_ACCESS_KEY_ID"),
    aws_secret_access_key=os.getenv("S3_SECRET_ACCESS_KEY"),
    region_name="auto", 
)
S3_BUCKET_NAME = os.getenv("S3_BUCKET_NAME")

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

def send_alert(message):
    if not SLACK_WEBHOOK_URL:
        logging.warning("SLACK_WEBHOOK_URL tanÄ±mlÄ± deÄŸil. Bildirim atlanÄ±yor.")
        return
    try:
        payload = {"text": f"ðŸš¨ **GPNAI Servis UyarÄ±sÄ±** ðŸš¨\n\n```{message}```"}
        requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=5)
    except Exception as e:
        logging.error(f"Slack bildirimi gÃ¶nderilemedi: {e}")

if not GEMINI_API_KEY or not S3_BUCKET_NAME:
    error_msg = "âŒ .env dosyasÄ±nda GEMINI_API_KEY veya S3 bilgileri eksik!"
    send_alert(error_msg) 
    raise ValueError(error_msg)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("scraper.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

def create_session():
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    session.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
    })
    return session

# --- YENÄ° EKLENDÄ°: Hash YardÄ±mcÄ± FonksiyonlarÄ± (Ã–neri 1.3) ---
def get_hash_from_s3(safe_name):
    """Mevcut hash'i S3'ten okur."""
    hash_key = f"{safe_name}_latest.hash"
    try:
        response = s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=hash_key)
        return response['Body'].read().decode('utf-8')
    except s3_client.exceptions.NoSuchKey:
        return None # Dosya henÃ¼z yok
    except Exception as e:
        logging.warning(f"S3'ten hash okuma hatasÄ± ({hash_key}): {e}")
        return None

def save_hash_to_s3(safe_name, new_hash):
    """Yeni hash'i S3'e yazar."""
    hash_key = f"{safe_name}_latest.hash"
    try:
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=hash_key,
            Body=new_hash.encode('utf-8'),
            ContentType="text/plain"
        )
    except Exception as e:
        logging.error(f"S3'e hash yazma hatasÄ± ({hash_key}): {e}")
        send_alert(f"âŒ S3'e hash yazma hatasÄ± ({hash_key}): {e}")
# -------------------------------------------------------------

def save_json_to_s3(data, base_name):
    filename = f"{base_name}_latest.json"
    try:
        json_string = json.dumps(data, indent=2, ensure_ascii=False)
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=filename,
            Body=json_string,
            ContentType="application/json"
        )
        logging.info(f"âœ… JSON S3'e kaydedildi: {S3_BUCKET_NAME}/{filename}")
    except Exception as e:
        logging.error(f"âŒ S3'e yazma hatasÄ± ({filename}): {e}")
        send_alert(f"âŒ S3'e yazma hatasÄ± ({filename}): {e}")

# --- GÃœNCELLENDÄ°: Paralel Veri Ã‡ekme (ArtÄ±k Hash KontrolÃ¼ YapÄ±yor) ---
def fetch_game_data(game_config, session):
    """
    Bir oyunun verisini Ã§eker ve hash'ini kontrol eder.
    DÃ¶nÃ¼ÅŸ: (game_name, raw_data, config, new_hash_or_skip_flag)
    """
    game_name = game_config['game']
    safe_name = game_config['safe_name']
    fetch_function_name = game_config['fetch_function']
    
    try:
        logging.info(f"THREAD ðŸ”: {game_name} iÃ§in veri Ã§ekiliyor...")
        fetch_function = getattr(scrapers, fetch_function_name)
        raw_data = fetch_function(session)
        
        if not raw_data:
            logging.warning(f"THREAD âš ï¸: {game_name} iÃ§in veri bulunamadÄ±.")
            return game_name, None, game_config, None # Hata/Fallback durumu

        # Hash KontrolÃ¼ (Ã–neri 1.3)
        new_hash = hashlib.sha256(raw_data.encode('utf-8')).hexdigest()
        old_hash = get_hash_from_s3(safe_name)
        
        if new_hash == old_hash:
            logging.info(f"THREAD â©: {game_name} verisi deÄŸiÅŸmemiÅŸ. Gemini analizi atlanÄ±yor.")
            return game_name, raw_data, game_config, "SKIPPED" # Veri deÄŸiÅŸmemiÅŸ
        
        return game_name, raw_data, game_config, new_hash # Veri yeni
        
    except Exception as e:
        logging.error(f"THREAD âŒ: {game_name} veri Ã§ekme hatasÄ±: {e}")
        return game_name, None, game_config, None # Hata durumu
# ------------------------------------------------------------------

# --- YENÄ° EKLENDÄ°: SaÄŸlÄ±k KontrolÃ¼ FonksiyonlarÄ± (Ã–neri 1.1) ---
def validate_selector(url, selector, parser_type='html.parser'):
    """Bir URL'den bir HTML/XML seÃ§icisinin varlÄ±ÄŸÄ±nÄ± kontrol eder."""
    try:
        res = requests.get(url, timeout=15, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0 Safari/537.36"
        })
        res.raise_for_status() # HTTP hatasÄ± varsa dur
        soup = BeautifulSoup(res.text, parser_type)
        
        # 'selector' CSS seÃ§ici mi yoksa 'find' argÃ¼manÄ± mÄ± olduÄŸuna gÃ¶re kontrol et
        # Bu Ã¶rnekte basitlik iÃ§in 'find' kullandÄ±ÄŸÄ±mÄ±zÄ± varsayalÄ±m (Ã¶rn: "div,class_=news-item-content")
        # Not: GerÃ§ek bir 'selector health check' daha karmaÅŸÄ±k bir 'find' argÃ¼manÄ± gerektirir.
        # Åžimdilik, sadece sitenin eriÅŸilebilir olduÄŸunu kontrol edelim (basitlik iÃ§in).
        if res.status_code == 200:
             return True # Temel kontrol: Site ayakta mÄ±?
        
        # GerÃ§ek seÃ§ici testi iÃ§in (daha karmaÅŸÄ±k):
        # if "," in selector:
        #     tag, attrs_str = selector.split(",", 1)
        #     attrs = dict([pair.split("=") for pair in attrs_str.split(",")])
        #     element = soup.find(tag, attrs)
        #     return element is not None
        # else:
        #     return soup.select_one(selector) is not None
        
    except Exception as e:
        logging.error(f"SaÄŸlÄ±k KontrolÃ¼ HatasÄ± ({url}): {e}")
        return False
    return False # VarsayÄ±lan olarak baÅŸarÄ±sÄ±z

def run_health_check():
    """TÃ¼m YAML kaynaklarÄ±nÄ± kontrol eder ve bozuksa uyarÄ±r."""
    logging.info("ðŸ©º Proaktif SaÄŸlÄ±k KontrolÃ¼ (Selector Health Check) baÅŸlÄ±yor...")
    try:
        with open("sources.yaml", "r", encoding="utf-8") as f:
            games_config = yaml.safe_load(f)
    except FileNotFoundError:
        send_alert("CRITICAL (Health Check): `sources.yaml` dosyasÄ± bulunamadÄ±!")
        return

    broken_selectors = []
    
    # Not: Bu, ÅŸu anki 'scrapers.py' yapÄ±mÄ±za gÃ¶re deÄŸil, 'sources.yaml'a eklenecek
    # 'check_url' ve 'check_selector' alanlarÄ±na gÃ¶re Ã§alÄ±ÅŸmalÄ±dÄ±r.
    # Faz 2 YAML'Ä±mÄ±zda bu alanlar yok.
    # Åžimdilik bu testi basitleÅŸtirip, sadece `fetch_` fonksiyonlarÄ±nÄ± Ã§aÄŸÄ±rÄ±p 
    # `None` dÃ¶nÃ¼p dÃ¶nmediÄŸini kontrol edelim.
    
    logging.info("SaÄŸlÄ±k kontrolÃ¼ iÃ§in kaynaklar Ã§ekiliyor (bu iÅŸlem biraz sÃ¼rebilir)...")
    session = create_session()
    
    for config in games_config:
        game_name = config['game']
        fetch_function_name = config['fetch_function']
        try:
            fetch_function = getattr(scrapers, fetch_function_name)
            data = fetch_function(session) # Test amaÃ§lÄ± veriyi Ã§ek
            if data is None:
                # Veri yoksa, bu bir fallback DEÄžÄ°L, scraper hatasÄ± olabilir.
                # (Valorant'taki 'veri yok' uyarÄ±sÄ± normal, ancak diÄŸerleri hata olabilir)
                logging.warning(f"HEALTH âš ï¸: {game_name} scraper'Ä± 'None' dÃ¶ndÃ¼rdÃ¼. Muhtemelen seÃ§ici bozuldu.")
                broken_selectors.append(game_name)
        except Exception as e:
            logging.error(f"HEALTH âŒ: {game_name} scraper'Ä± test sÄ±rasÄ±nda Ã§Ã¶ktÃ¼: {e}")
            broken_selectors.append(f"{game_name} (Ã‡Ã¶ktÃ¼)")
            
    session.close()

    if broken_selectors:
        send_alert(f"âŒ PROAKTÄ°F UYARI: Åžu scraper'lar bozulmuÅŸ olabilir:\n- " + "\n- ".join(broken_selectors))
    else:
        logging.info("âœ… SaÄŸlÄ±k KontrolÃ¼ tamamlandÄ±. TÃ¼m scraper'lar Ã§alÄ±ÅŸÄ±r durumda.")
        # BaÅŸarÄ±lÄ± olursa sessiz kal
        # send_alert("âœ… SaÄŸlÄ±k KontrolÃ¼ tamamlandÄ±. TÃ¼m scraper'lar Ã§alÄ±ÅŸÄ±r durumda.")
# ---------------------------------------------------------------

def run_scrape():
    """Ana veri Ã§ekme ve analiz iÅŸlemini Ã§alÄ±ÅŸtÄ±rÄ±r."""
    logging.info("ðŸš€ Faz 3: Hash KontrollÃ¼ Paralel Veri Ã‡ekme ve SÄ±ralÄ± Analiz baÅŸlÄ±yor...")
    
    try:
        with open("sources.yaml", "r", encoding="utf-8") as f:
            games_config = yaml.safe_load(f)
        
        session = create_session()
        fetched_data = [] 

        with concurrent.futures.ThreadPoolExecutor(max_workers=len(games_config)) as executor:
            futures = [executor.submit(fetch_game_data, config, session) for config in games_config]
            
            for future in concurrent.futures.as_completed(futures):
                fetched_data.append(future.result())

        logging.info(f"âœ… Paralel veri Ã§ekme tamamlandÄ±. {len(fetched_data)} oyun iÅŸlenecek.")
        session.close() 

        # SÄ±ralÄ± Analiz (API Limiti KorumasÄ±)
        for i, (game_name, raw_data, config, hash_or_flag) in enumerate(fetched_data):
            
            if hash_or_flag == "SKIPPED":
                continue # Hash aynÄ±, bu oyunu atla
            
            safe_name = config['safe_name']

            if not raw_data:
                fallback = f"{game_name} received balance changes and new content."
                logging.warning(f"âš ï¸  {game_name} iÃ§in veri yok. Fallback metin kullanÄ±lÄ±yor.")
                raw_data = fallback
            else:
                logging.info(f"ANALÄ°Z ðŸ§ : {game_name} verisi iÅŸleniyor (Hash: {hash_or_flag[:7]}...).")

            result = analyze_with_gemini(raw_data, game_name, send_alert)
            
            if result:
                save_json_to_s3(result, safe_name)
                # Sadece analiz ve S3 kaydÄ± baÅŸarÄ±lÄ±ysa yeni hash'i kaydet
                if hash_or_flag not in [None, "SKIPPED"]:
                    save_hash_to_s3(safe_name, hash_or_flag)
            else:
                logging.error(f"âŒ {game_name} analizi baÅŸarÄ±sÄ±z.")
            
            if i < len(fetched_data) - 1:
                delay = random.uniform(5, 12)
                logging.info(f"â³ Gemini rate limit korumasÄ± iÃ§in {delay:.1f} saniye bekleniyor...")
                time.sleep(delay)
        
        logging.info("âœ… TÃ¼m oyunlarÄ±n yama analizi baÅŸarÄ±yla tamamlandÄ±.")
        
    except Exception as e:
        logging.error(f"CRITICAL: Cron Job'da beklenmedik hata: {e}", exc_info=True)
        send_alert(f"CRITICAL: Cron Job'un tamamÄ± Ã§Ã¶ktÃ¼: {e}")

# --- YENÄ° EKLENDÄ°: Ana Ã‡alÄ±ÅŸtÄ±rma MantÄ±ÄŸÄ± (Mod SeÃ§imi) ---
if __name__ == "__main__":
    # Komut satÄ±rÄ±ndan argÃ¼manlarÄ± oku (Ã¶rn: python scrape.py --run=health)
    args = dict(arg.split('=') for arg in sys.argv[1:] if '=' in arg)
    run_mode = args.get('--run', 'scrape') # VarsayÄ±lan mod 'scrape'

    if run_mode == 'health':
        run_health_check()
    elif run_mode == 'scrape':
        run_scrape()
    else:
        logging.error(f"GeÃ§ersiz Ã§alÄ±ÅŸma modu: {run_mode}. '--run=scrape' veya '--run=health' kullanÄ±n.")
# ---------------------------------------------------------